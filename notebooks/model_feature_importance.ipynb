{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10edd13",
   "metadata": {},
   "source": [
    "# Model Feature Importance Analysis\n",
    "This notebook analyzes a saved PyTorch `.pth` model and computes feature importance using permutation importance, SHAP, and Integrated Gradients. It is designed to run inside the current workspace and save visualizations and results locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies and Configure Environment\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    torch.manual_seed(42)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "except Exception as e:\n",
    "    print('PyTorch unavailable:', e)\n",
    "    torch = None\n",
    "    nn = None\n",
    "    device = 'cpu'\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "except Exception as e:\n",
    "    print('scikit-learn unavailable:', e)\n",
    "try:\n",
    "    import shap\n",
    "except Exception as e:\n",
    "    print('SHAP unavailable:', e)\n",
    "    shap = None\n",
    "try:\n",
    "    from captum.attr import IntegratedGradients\n",
    "except Exception as e:\n",
    "    print('Captum unavailable:', e)\n",
    "    IntegratedGradients = None\n",
    "from datetime import datetime\n",
    "fig_dir = pathlib.Path('notebooks/figures')\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Figures will be saved to:\", fig_dir)\n",
    "print(\"Workspace root:\", pathlib.Path('.').resolve())\n",
    "print(\"Variance-based importance idea: I_j = E[(ŷ(X) - ŷ(X^(j perm)))^2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b888d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and Define Feature Columns\n",
    "data_path = pathlib.Path('data/features.csv')\n",
    "label_column = os.environ.get('LABEL_COLUMN', 'label')\n",
    "feature_columns: Optional[List[str]] = None\n",
    "if not data_path.exists():\n",
    "    print(f\"Dataset not found at {data_path}. Please place your features CSV there.\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print('Dataset shape:', df.shape)\n",
    "    if label_column in df.columns:\n",
    "        y = df[label_column].values\n",
    "        X = df.drop(columns=[label_column])\n",
    "    else:\n",
    "        print(f\"Label column '{label_column}' not found. Treating as unlabeled features.\")\n",
    "        X = df\n",
    "        y = None\n",
    "    feature_columns = list(X.columns)\n",
    "    print('Feature columns:', len(feature_columns))\n",
    "    # Train/Val/Test split if labels available\n",
    "    if y is not None:\n",
    "        X_train, X_tmp, y_train, y_tmp = train_test_split(X.values, y, test_size=0.4, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "    else:\n",
    "        X_train, X_val, X_test = X.values, None, X.values\n",
    "        y_train, y_val, y_test = None, None, None\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val) if X_val is not None else None\n",
    "    X_test_scaled = scaler.transform(X_test) if X_test is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35737bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .pth Model and Prepare for Inference\n",
    "model_path = pathlib.Path(os.environ.get('MODEL_PATH', 'models/model.pth'))\n",
    "class SimpleTabularNN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 64, out_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "def load_model(in_dim: int, out_dim: int = 1):\n",
    "    if torch is None:\n",
    "        print('PyTorch not available; skipping model load.')\n",
    "        return None\n",
    "    model = SimpleTabularNN(in_dim, hidden=64, out_dim=out_dim)\n",
    "    if model_path.exists():\n",
    "        sd = torch.load(model_path, map_location=device)\n",
    "        # Handle either state_dict or entire model\n",
    "        if isinstance(sd, dict) and 'state_dict' in sd:\n",
    "            model.load_state_dict(sd['state_dict'])\n",
    "        elif isinstance(sd, dict):\n",
    "            try:\n",
    "                model.load_state_dict(sd)\n",
    "            except Exception as e:\n",
    "                print('State dict load failed:', e)\n",
    "        else:\n",
    "            print('Unknown checkpoint format; expected state_dict dict.')\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "def infer(model, X_np: np.ndarray):\n",
    "    if model is None or torch is None:\n",
    "        return None\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
    "        out = model(x).detach().cpu().numpy()\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5243a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Evaluation Metrics\n",
    "task_type = os.environ.get('TASK_TYPE', 'regression')  # 'classification' or 'regression'\n",
    "metrics = {}\n",
    "if 'X_test_scaled' in globals() and X_test_scaled is not None:\n",
    "    model = load_model(in_dim=X_test_scaled.shape[1], out_dim=1)\n",
    "    y_pred = infer(model, X_test_scaled)\n",
    "    if y_pred is not None:\n",
    "        if task_type == 'classification' and y_test is not None:\n",
    "            # Assume binary classification with logistic output; apply sigmoid\n",
    "            y_prob = 1 / (1 + np.exp(-y_pred))\n",
    "            y_hat = (y_prob >= 0.5).astype(int)\n",
    "            metrics['accuracy'] = accuracy_score(y_test, y_hat)\n",
    "            try:\n",
    "                metrics['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif task_type == 'regression' and y_test is not None:\n",
    "            metrics['mse'] = mean_squared_error(y_test, y_pred)\n",
    "            metrics['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "        print('Baseline metrics:', json.dumps(metrics, indent=2))\n",
    "    else:\n",
    "        print('Inference failed (no predictions).')\n",
    "else:\n",
    "    print('Test set unavailable; skipping baseline metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "def metric_func(y_true, y_pred):\n",
    "    if y_true is None or y_pred is None:\n",
    "        return np.nan\n",
    "    if task_type == 'classification':\n",
    "        y_prob = 1 / (1 + np.exp(-y_pred))\n",
    "        return roc_auc_score(y_true, y_prob)\n",
    "    else:\n",
    "        return -mean_squared_error(y_true, y_pred)\n",
    "perm_results = None\n",
    "if 'X_test_scaled' in globals() and X_test_scaled is not None and feature_columns is not None:\n",
    "    K = 5  # repeats\n",
    "    base_pred = y_pred\n",
    "    base_metric = metric_func(y_test, base_pred) if y_test is not None else np.nan\n",
    "    deltas = []\n",
    "    for j, col in enumerate(feature_columns):\n",
    "        scores = []\n",
    "        for k in range(K):\n",
    "            Xp = X_test_scaled.copy()\n",
    "            np.random.shuffle(Xp[:, j])\n",
    "            yp = infer(model, Xp)\n",
    "            m = metric_func(y_test, yp) if y_test is not None else np.nan\n",
    "            scores.append(base_metric - m)\n",
    "        deltas.append({'feature': col, 'importance': float(np.nanmean(scores))})\n",
    "    perm_results = pd.DataFrame(deltas).sort_values('importance', ascending=False)\n",
    "    print('Permutation importance top 10:')\n",
    "    print(perm_results.head(10))\n",
    "else:\n",
    "    print('Permutation importance skipped (missing test set or feature names).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Feature Importance (Kernel/Deep Explainer)\n",
    "shap_values = None\n",
    "shap_summary = None\n",
    "if shap is not None and model is not None and X_test_scaled is not None:\n",
    "    try:\n",
    "        sample_idx = np.random.choice(np.arange(X_test_scaled.shape[0]), size=min(256, X_test_scaled.shape[0]), replace=False)\n",
    "        X_bg = X_train_scaled[np.random.choice(np.arange(X_train_scaled.shape[0]), size=min(128, X_train_scaled.shape[0]), replace=False)]\n",
    "        X_sample = X_test_scaled[sample_idx]\n",
    "        def f_predict(Xnp):\n",
    "            return infer(model, Xnp)\n",
    "        try:\n",
    "            explainer = shap.DeepExplainer(model, torch.tensor(X_bg, dtype=torch.float32, device=device))\n",
    "            sv = explainer.shap_values(torch.tensor(X_sample, dtype=torch.float32, device=device))\n",
    "            shap_values = sv if isinstance(sv, np.ndarray) else sv[0]\n",
    "        except Exception as _deep_e:\n",
    "            explainer = shap.KernelExplainer(f_predict, X_bg)\n",
    "            shap_values = explainer.shap_values(X_sample, nsamples=200)\n",
    "        shap_values = np.array(shap_values)\n",
    "        shap_summary = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'mean_abs_shap': np.mean(np.abs(shap_values), axis=0)[:len(feature_columns)]\n",
    "        }).sort_values('mean_abs_shap', ascending=False)\n",
    "        print('SHAP top 10:')\n",
    "        print(shap_summary.head(10))\n",
    "    except Exception as e:\n",
    "        print('SHAP failed:', e)\n",
    "else:\n",
    "    print('SHAP not run (missing SHAP, model, or data).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients with Captum\n",
    "ig_summary = None\n",
    "if IntegratedGradients is not None and model is not None and X_test_scaled is not None:\n",
    "    try:\n",
    "        ig = IntegratedGradients(model)\n",
    "        batch = torch.tensor(X_test_scaled[:256], dtype=torch.float32, device=device)\n",
    "        baseline = torch.zeros_like(batch)\n",
    "        attributions, _ = ig.attribute(inputs=batch, baselines=baseline, target=None, return_convergence_delta=True)\n",
    "        attributions = attributions.detach().cpu().numpy()\n",
    "        ig_scores = np.mean(np.abs(attributions), axis=0)\n",
    "        ig_summary = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'mean_abs_ig': ig_scores[:len(feature_columns)]\n",
    "        }).sort_values('mean_abs_ig', ascending=False)\n",
    "        print('Integrated Gradients top 10:')\n",
    "        print(ig_summary.head(10))\n",
    "    except Exception as e:\n",
    "        print('Captum IntegratedGradients failed:', e)\n",
    "else:\n",
    "    print('Captum IG not run (missing Captum, model, or data).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28687cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate, Rank, and Visualize Importances\n",
    "all_tables = []\n",
    "if perm_results is not None:\n",
    "    all_tables.append(perm_results.rename(columns={'importance': 'perm_importance'}))\n",
    "if shap_summary is not None:\n",
    "    all_tables.append(shap_summary.rename(columns={'mean_abs_shap': 'shap_importance'}))\n",
    "if ig_summary is not None:\n",
    "    all_tables.append(ig_summary.rename(columns={'mean_abs_ig': 'ig_importance'}))\n",
    "combined = None\n",
    "if all_tables:\n",
    "    combined = all_tables[0]\n",
    "    for t in all_tables[1:]:\n",
    "        combined = combined.merge(t, on='feature', how='outer')\n",
    "    # Normalize each column to [0,1]\n",
    "    for col in ['perm_importance','shap_importance','ig_importance']:\n",
    "        if col in combined:\n",
    "            m = combined[col].max()\n",
    "            if pd.notnull(m) and m > 0:\n",
    "                combined[col] = combined[col] / m\n",
    "    combined['avg_importance'] = combined[[c for c in ['perm_importance','shap_importance','ig_importance'] if c in combined]].mean(axis=1)\n",
    "    combined = combined.sort_values('avg_importance', ascending=False)\n",
    "    print('Combined importance (top 20):')\n",
    "    print(combined.head(20))\n",
    "    # Visualize\n",
    "    top_n = combined.head(25)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=top_n, x='avg_importance', y='feature', orient='h')\n",
    "    plt.title('Feature Importance (Aggregated)')\n",
    "    plt.tight_layout()\n",
    "    out_bar = fig_dir / 'importance_aggregated_bar.png'\n",
    "    plt.savefig(out_bar)\n",
    "    print('Saved:', out_bar)\n",
    "    # Correlation heatmap among methods\n",
    "    cols = [c for c in ['perm_importance','shap_importance','ig_importance'] if c in combined]\n",
    "    if len(cols) >= 2:\n",
    "        plt.figure(figsize=(6,5))\n",
    "        corr = combined[cols].corr()\n",
    "        sns.heatmap(corr, annot=True, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "        plt.title('Importance Methods Correlation')\n",
    "        plt.tight_layout()\n",
    "        out_heat = fig_dir / 'importance_methods_corr.png'\n",
    "        plt.savefig(out_heat)\n",
    "        print('Saved:', out_heat)\n",
    "else:\n",
    "    print('No importance tables to aggregate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efed313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Artifacts and Results to Disk\n",
    "results_dir = pathlib.Path('notebooks/results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "config = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'device': str(device),\n",
    "    'model_path': str(model_path),\n",
    "    'data_path': str(data_path),\n",
    "    'task_type': task_type,\n",
    "    'label_column': label_column,\n",
    "    'feature_count': len(feature_columns) if feature_columns else 0,\n",
    "}\n",
    "(results_dir / 'config.json').write_text(json.dumps(config, indent=2))\n",
    "if perm_results is not None:\n",
    "    perm_results.to_csv(results_dir / 'importance_permutation.csv', index=False)\n",
    "if shap_summary is not None:\n",
    "    shap_summary.to_csv(results_dir / 'importance_shap.csv', index=False)\n",
    "if ig_summary is not None:\n",
    "    ig_summary.to_csv(results_dir / 'importance_integrated_gradients.csv', index=False)\n",
    "if 'combined' in globals() and combined is not None:\n",
    "    combined.to_csv(results_dir / 'importance_combined.csv', index=False)\n",
    "print('Saved artifacts to:', results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
